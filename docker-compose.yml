version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: nyc-taxi-case-zookeeper-1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181


  kafka:
    image: bitnami/kafka:3.4.0
    container_name: nyc-taxi-case-kafka-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_KRAFT_MODE: "false"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: ""
      KAFKA_KRAFT_CLUSTER_ID: ""
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list || exit 1"]
      interval: 10s
      retries: 5
      start_period: 30s
    volumes:
      - kafka_data:/bitnami/kafka/data  # Persistent data volume
    restart: always


  producer:
    build: ./producer
    container_name: nyc-taxi-case-producer-1
    depends_on:
      - kafka
    volumes:
      - ${PROJECT_ROOT}/data:/app/data
      - ${PROJECT_ROOT}/schemas:/app/schemas
    env_file:
      - .env


  spark:
    image: bitnami/spark:3.3
    container_name: nyc-taxi-case-spark-1
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/datalake/spark-events
      - SPARK_DAEMON_MEMORY=1g
    command: >
      bash -c "
        /opt/bitnami/spark/sbin/start-master.sh && 
        /opt/bitnami/spark/sbin/start-worker.sh spark://spark:7077 && 
        /opt/bitnami/spark/sbin/start-history-server.sh && 
        tail -f /dev/null"
    volumes:
      - ./datalake/spark-events:/datalake/spark-events
    ports:
      - "7077:7077"  # Porta de comunicação do Spark
      - "18080:18080"  # UI do Spark History Server


  streaming-consumer:
    image: bitnami/spark:3.3
    container_name: nyc-taxi-case-streaming-consumer-1
    depends_on:
      - spark
      - kafka
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark-master:7077
    entrypoint: ["/bin/bash", "-c"]
    command: >
      "/opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --deploy-mode client
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0
      --class nyc.taxi.consumer.TaxiConsumer
      /opt/bitnami/spark/jobs/taxiconsumer_2.12-1.0.jar"
    volumes:
      - ./consumer/target/scala-2.12:/opt/bitnami/spark/jobs
      - ./datalake:/datalake"
  

  postgres:
    image: postgres:13
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5


  airflow:
    build:
      context: ./docker/airflow  # Path to the new Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: ["/bin/bash", "-c", "pip install apache-airflow-providers-apache-kafka && airflow db init && airflow webserver"]
    command: >
      "pip install --no-cache-dir apache-airflow-providers-apache-spark &&
      airflow db init &&
      airflow webserver"
    volumes:
      - ${PROJECT_ROOT}/dags:/opt/airflow/dags
    ports:
      - "8080:8080"
  
  airflow-scheduler:
    image: apache/airflow:2.5.1
    depends_on:
      - postgres
      - airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: ["/bin/bash", "-c", "airflow db init && airflow scheduler"]
    volumes:
      - ${PROJECT_ROOT}/dags:/opt/airflow/dags
    restart: always


volumes:
  postgres_data:
  kafka_data: