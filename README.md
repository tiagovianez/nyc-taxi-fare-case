# Case de Engenharia de Dados  - EITS MLOps Engineering - Serasa Experian




## Para ativar vari√°vel de ambiente:
source venv/bin/activate


1. Primeiro suba os servi√ßos utilizando o docker compose file utilizando o terminal: **"docker compose up --build -d"**
2. Crie um t√≥pico kafka utilizando executando esse comando: **docker exec -it kafka sh -c "/opt/bitnami/kafka/bin/kafka-topics.sh --create --topic nyc-taxi-rides --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"**
3. Ap√≥s isso, liste os t√≥picos no container do Kafka via docker : **docker exec -it kafka sh -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"**. Isso ser√° interessante para validar que o t√≥pico kafka que ir√° receber as mensagens est√° dispon√≠vel.
4. Checado isso, v√° at√© o diret√≥rio "**/nyc-taxi-fare-case/nyctaxi**" via terminal e execute o comando para startar o Producer **sbt "runMain fare.nyctaxi.producer.ProducerJob"**
5. [Opicional] Caso voc√™ necessite checar os eventos do t√≥pico kafka, s√≥ executar o comando no terminal: **docker exec -it kafka sh -c "/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic nyc-taxi-rides"**. Isso ir√° permitir que voc√™ consiga visualizar as novas mensagens enviadas.


## Comandos de execu√ß√£o e valida√ß√£o
- Para executar o producer: **sbt "runMain fare.nyctaxi.producer.KafkaTaxiProducer"**
- Para executar o consumer: **sbt "runMain fare.nyctaxi.consumer.TaxiConsumer"**


## Valida√ß√µes dentro do container do Kafka
- Primeiro, valide que todos os containers necess√°rios est√£o de p√©: **docker ps**
- Acesso o container do Kafka utilizando o comando: **docker exec -it kafka bash**
- Uma vez dentro do container do Kafka j√° ser√° poss√≠vel realizar algumas valida√ß√µes do tipo:
- Liste todos os t√≥picos no Kafka: **kafka-topics.sh --bootstrap-server localhost:9092 --list**
- Caso n√£o exista nenhum t√≥pico e voc√™ deseja cri√°-lo, s√≥ executar: **kafka-topics.sh --bootstrap-server localhost:9092 --create --topic nyc-taxi-rides --partitions 3 --replication-factor 1**
- Para testar a conex√£o do Kafka com o Producer




## üìå Como fica a estrutura no Data Lake?
![img.png](img.png)

## üöÄ Vantagens dessa abordagem
‚úÖ Consultas r√°pidas ‚Üí Spark l√™ apenas os arquivos necess√°rios
‚úÖ Menos armazenamento desperdi√ßado ‚Üí Evita pequenos arquivos dispersos
‚úÖ Escalabilidade ‚Üí Aguenta grandes volumes de dados sem travar
‚úÖ Integra√ß√£o com batch e NRT ‚Üí Pode rodar queries anal√≠ticas e streaming




## Analysys memory

### amout of data stored in the raw: 354.8MB
### amount of data stored in the curated: 457.8MB



## Airfllow comandos
üîπ Comandos Airflow

**airflow db upgrade** ‚Üí Atualiza o banco de dados.
**airflow users create** ... ‚Üí Cria um usu√°rio administrador.
**airflow scheduler** & **airflow webserver** ‚Üí Inicia os servi√ßos do Airflow.
**docker logs -f nyc-taxi-fare-case-airflow-1** -> Verifica as logs do airflow


## Comandos do docker
**docker volume ls** -> Verifica se ainda tem volume ativo
**docker volume rm <service_data>** -> remove manualmente o volume do servi√ßo




## SBT configura√ß√µes:

1. Configurar SBT_OPTS para aumentar a mem√≥ria

Antes de rodar o sbt assembly, execute o seguinte comando no terminal para aumentar a mem√≥ria dispon√≠vel:

export SBT_OPTS="-Xms2G -Xmx4G -XX:MaxMetaspaceSize=1G -XX:+UseG1GC"


## Como o Delta Lake lida com Escritas em Streaming?

O Delta Lake foi projetado para suportar leitura e escrita simult√¢neas, garantindo Atomicidade, Consist√™ncia, Isolamento e Durabilidade (ACID). Isso significa que:

‚úÖ Nenhum analista ou cientista de dados ver√° dados incompletos (porque o Delta faz commit de transa√ß√µes atomizadas).
‚úÖ Os dados sempre estar√£o dispon√≠veis para leitura sem conflitos.
‚úÖ Leitores e Escritores podem trabalhar ao mesmo tempo, pois o Delta usa arquivos transacionais para controle de vers√£o.


### Para ler tabelas em Delta utilizando o Spark/Scala local da m√°quina:
Execute esse comando: spark-shell --packages io.delta:delta-core_2.12:2.4.0




### Como Melhorar a Efici√™ncia?

Para garantir que ningu√©m tenha impacto negativo na performance, algumas boas pr√°ticas podem ser aplicadas:
1Ô∏è‚É£ Ativar Auto-Optimize (Compacta√ß√£o)

Como o streaming grava muitos pequenos arquivos, ativar Auto-Optimize e Auto-Compaction no Delta ajuda a evitar fragmenta√ß√£o:

ALTER TABLE delta.`/path/to/curated` SET TBLPROPERTIES (
'delta.autoOptimize.optimizeWrite' = 'true',
'delta.autoOptimize.autoCompact' = 'true'
);

**Benef√≠cio**:
Reduz a fragmenta√ß√£o e melhora a leitura sem afetar o streaming.

###  Conclus√£o

‚úÖ Sim, a leitura pode acontecer sem impacto para analistas e cientistas de dados.
‚úÖ Delta Lake garante consist√™ncia dos dados mesmo com escrita em streaming.
‚úÖ Ativar auto-compacta√ß√£o melhora a performance e reduz fragmenta√ß√£o.
‚úÖ Materialized Views podem desacoplar leitura e escrita.

Se voc√™ seguir essas pr√°ticas, seu pipeline ser√° s√≥lido e escal√°vel, garantindo que ningu√©m seja impactado pela escrita cont√≠nua! üöÄüî•




## Docker do Airflow
### Ps.: Se certifique que voc√™ esteja no diret√≥rio do docker
Para subir os servi√ßos do airflow via docker: **docker compose -f docker-compose.Airflow.yml up -d --build**


#### Reinicializa√ß√£o do airflow via docker: 
**docker compose -f docker-compose.Airflow.yml down
docker compose -f docker-compose.Airflow.yml up -d**



### Para limpar espa√ßo no docker:
**docker system prune -a --volumes**

